{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N=3 100回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Trial 1 / 100 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulation Progress:   0%|          | 40831/100000000 [00:05<3:34:57, 7750.06it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 268\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m trial \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m101\u001b[39m):  \u001b[38;5;66;03m# 100回シミュレーション \u001b[39;00m\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Trial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / 100 ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 268\u001b[0m     results, prob_history, payoff_history, s \u001b[38;5;241m=\u001b[39m run_simulation(N, NTIME, a, b, c, xmax, chi, kappa, trial)\n",
      "Cell \u001b[1;32mIn[2], line 183\u001b[0m, in \u001b[0;36mrun_simulation\u001b[1;34m(N, NTIME, a, b, c, xmax, chi, kappa, trial)\u001b[0m\n\u001b[0;32m    181\u001b[0m payoff_history\u001b[38;5;241m.\u001b[39mappend(current_payoffs)  \u001b[38;5;66;03m# 各時点のエージェントの利得を追加\u001b[39;00m\n\u001b[0;32m    182\u001b[0m actions_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, [agent\u001b[38;5;241m.\u001b[39maction \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m agents]))\n\u001b[1;32m--> 183\u001b[0m file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mS0\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSmj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactions_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mqarray_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimproved\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m improved:\n\u001b[0;32m    186\u001b[0m     s \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from matplotlib import cm\n",
    "\n",
    "# エージェントの基本クラス\n",
    "class Agent:\n",
    "    def __init__(self, initial_action):\n",
    "        self.action = initial_action\n",
    "        self.total_payoff = 0.0\n",
    "\n",
    "    def update_payoff(self, payoff):\n",
    "        self.total_payoff += payoff\n",
    "\n",
    "# ZD戦略を持つエージェントクラス\n",
    "class ZDAgent(Agent):\n",
    "    def __init__(self, x0ov, x0und, W, kappa, chi):\n",
    "        self.actions = {\"x_ov\": x0ov, \"x_und\": x0und}\n",
    "        initial_action_key = np.random.choice([\"x_ov\", \"x_und\"])\n",
    "        super().__init__(self.actions[initial_action_key])\n",
    "        self.W = W\n",
    "        self.kappa = kappa\n",
    "        self.chi = chi\n",
    "        self.current_action_key = initial_action_key\n",
    "\n",
    "    def choose_action(self, B):\n",
    "        T0 = self.calculate_probability(B)\n",
    "        if np.random.random() <= T0:\n",
    "            self.action = self.actions[\"x_und\"]\n",
    "            self.current_action_key = \"x_und\"\n",
    "        else:\n",
    "            self.action = self.actions[\"x_ov\"]\n",
    "            self.current_action_key = \"x_ov\"\n",
    "\n",
    "    def calculate_probability(self, B):\n",
    "        return B / self.W + 1.0 if self.current_action_key == \"x_und\" else B / self.W\n",
    "\n",
    "# 通常のエージェントクラス\n",
    "class NormalAgent(Agent):\n",
    "    def __init__(self, a, b, c, N):\n",
    "        self.actions = {\n",
    "            \"xc\": (a - c) / (2 * N * b),\n",
    "            \"xn\": (a - c) / ((N + 1) * b)\n",
    "        }\n",
    "        initial_action_key = \"xc\" if np.random.random() < 0.5 else \"xn\"\n",
    "        super().__init__(self.actions[initial_action_key])\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "        self.N = N\n",
    "        self.current_action_key = initial_action_key\n",
    "        self.qarray = np.random.rand(2*2*(N-1))\n",
    "\n",
    "    def calc_payoff(self, qarray, agents, number, a, b, c):\n",
    "        actions = []\n",
    "        if qarray.all == self.qarray.all:\n",
    "            for i, agent in enumerate(agents):\n",
    "                actions.append(agent.action)\n",
    "            xtot = sum(actions)\n",
    "            # 利得を計算\n",
    "            return ((a - b * xtot) * theta(a - b * xtot) - c) * actions[number]\n",
    "        else:\n",
    "            for i, agent in enumerate(agents):\n",
    "                if i == number:\n",
    "                    # 自分の仮定行動を `qarray` に基づいて計算\n",
    "                    if np.random.random() < qarray[self.prev_action_index]:\n",
    "                        actions.append(self.actions[\"xc\"])\n",
    "                    else:\n",
    "                        actions.append(self.actions[\"xn\"])\n",
    "                else:\n",
    "                    actions.append(agent.action)\n",
    "            xtot = sum(actions)\n",
    "            # 利得を計算\n",
    "            return ((a - b * xtot) * theta(a - b * xtot) - c) * actions[number], actions[number]\n",
    "\n",
    "    def choose_action(self, zd_action, other_actions, zd_actions, agents, number, a, b, c, t, N):\n",
    "        zd_discrete = 0 if zd_action == zd_actions[\"x_und\"] else 1\n",
    "        self_discrete = 0 if self.action == self.actions[\"xc\"] else 1\n",
    "        other_count = sum(1 for action in other_actions if action == self.actions[\"xc\"])\n",
    "        self.prev_action_index = self_discrete *(N-1)*2 + zd_discrete *(N-1)  + other_count\n",
    "        #print(self.prev_action_index)\n",
    "\n",
    "        if t != 0:\n",
    "            if np.random.random() < self.qarray[self.prev_action_index]:\n",
    "                self.action = self.actions[\"xc\"]\n",
    "                self.current_action_key = \"xc\"\n",
    "            else:\n",
    "                self.action = self.actions[\"xn\"]\n",
    "                self.current_action_key = \"xn\"\n",
    "\n",
    "        \"\"\"ランダムにqarrayを変化させて期待利得を比較し、改善があれば採用する。\"\"\"\n",
    "        # qarrayの変更を作成 (+0.001 または -0.001)\n",
    "        delta = np.zeros_like(self.qarray)  # deltaの初期化\n",
    "        delta[self.prev_action_index] = np.random.choice([-0.01, 0.01])  # 指定されたインデックスだけ変更\n",
    "        new_qarray = np.clip(self.qarray + delta, 0.0, 1.0)  # [0, 1]の範囲に制限\n",
    "\n",
    "        # 期待利得を計算\n",
    "        current_payoff = self.calc_payoff(self.qarray, agents, number, a, b, c)\n",
    "        new_payoff, new_action = self.calc_payoff(new_qarray, agents, number, a, b, c)\n",
    "\n",
    "        # 改善があれば新しいqarrayを採用\n",
    "        if new_payoff > current_payoff:\n",
    "            self.qarray = new_qarray\n",
    "            self.action = new_action\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "# ステップ関数\n",
    "def theta(y):\n",
    "    return 1.0 if y >= 0 else 0.0\n",
    "\n",
    "# 各エージェントの利得を計算\n",
    "def calculate_payoffs(agents, a, b, c):\n",
    "    xtot = sum(agent.action for agent in agents)\n",
    "    for agent in agents:\n",
    "        payoff = ((a - b * xtot) * theta(a - b * xtot) - c) * agent.action\n",
    "        agent.update_payoff(payoff)\n",
    "\n",
    "# シミュレーション実行\n",
    "def run_simulation(N, NTIME, a, b, c, xmax, chi, kappa, trial):\n",
    "    results_1 = []\n",
    "    prob_history = []\n",
    "    payoff_history = []\n",
    "    W = 2.0 * c * xmax + abs((chi - 1) * kappa)\n",
    "    x0ov = chi / (chi + N - 1.0) * (a - c) / b\n",
    "    x0und = xmax\n",
    "    zd_actions = {\"x_ov\": x0ov, \"x_und\": x0und}\n",
    "    # 各アクションのカウントを初期化\n",
    "    action_counts = {\"x_ov\": 0, \"x_und\": 0, \"xc\": 0, \"xn\": 0}\n",
    "    agents = [ZDAgent(x0ov, x0und, W, kappa, chi)]\n",
    "    agents.extend(NormalAgent(a, b, c, N) for _ in range(1, N))\n",
    "\n",
    "    B = agents[0].total_payoff + (chi - 1.0) * kappa\n",
    "    for agent in agents[1:]:\n",
    "        B -= chi / (N - 1) * agent.total_payoff\n",
    "\n",
    "    s = 0\n",
    "    k = 0\n",
    "    # データ保存用\n",
    "    with open(f\"simulation_results_vs2_all_trial_{trial}.txt\", \"w\") as file:\n",
    "        file.write(\"TimeStep,S0,Smj_avg,Actions,qarray, imporoved, imporoved step\\n\")  # ヘッダー行\n",
    "        for t in tqdm(range(NTIME), desc=\"Simulation Progress\"):\n",
    "            previous_actions = [agent.action for agent in agents]\n",
    "            improved = False\n",
    "\n",
    "            for i in range(1, N):\n",
    "                other_actions = [previous_actions[j] for j in range(len(agents)) if j != i]\n",
    "                if agents[i].choose_action(agents[0].action, other_actions, zd_actions, agents, i, a, b, c, t, N):\n",
    "                    improved = True\n",
    "                    k = t\n",
    "                \n",
    "            qarray_str = \",\".join(map(str, np.mean([agent.qarray for agent in agents[1:]], axis=0)))\n",
    "\n",
    "            agents[0].choose_action(B)\n",
    "            calculate_payoffs(agents, a, b, c)\n",
    "            B = (\n",
    "                agents[0].total_payoff\n",
    "                - chi / (N - 1) * sum(agent.total_payoff for agent in agents[1:])\n",
    "                + (chi - 1.0) * kappa\n",
    "            )\n",
    "            # アクションをカウント\n",
    "            if agents[0].current_action_key == \"x_ov\":\n",
    "                action_counts[\"x_ov\"] += 1\n",
    "            elif agents[0].current_action_key == \"x_und\":\n",
    "                action_counts[\"x_und\"] += 1\n",
    "\n",
    "            for agent in agents[1:]:\n",
    "                if agent.current_action_key == \"xc\":\n",
    "                    action_counts[\"xc\"] += 1\n",
    "                elif agent.current_action_key == \"xn\":\n",
    "                    action_counts[\"xn\"] += 1\n",
    "\n",
    "            S0 = agents[0].action * ((a - b * sum(agent.action for agent in agents)) * theta(a - b * sum(agent.action for agent in agents)) - c)\n",
    "            Smj = np.mean([\n",
    "                agent.action * ((a - b * sum(agent.action for agent in agents)) * theta(a - b * sum(agent.action for agent in agents)) - c)\n",
    "                for agent in agents[1:]\n",
    "            ])\n",
    "            results_1.append((S0, Smj))\n",
    "\n",
    "            current_payoffs = [agent.action * ((a - b * sum(agent.action for agent in agents)) * theta(a - b * sum(agent.action for agent in agents)) - c) for agent in agents]\n",
    "            payoff_history.append(current_payoffs)  # 各時点のエージェントの利得を追加\n",
    "            actions_str = \",\".join(map(str, [agent.action for agent in agents]))\n",
    "            file.write(f\"{t},{S0},{Smj},{actions_str},{qarray_str}, {improved}, {k}\\n\")\n",
    "\n",
    "            if improved:\n",
    "                s += 1\n",
    "                prob_history.append(np.mean([agent.qarray for agent in agents[1:]], axis=0))\n",
    "\n",
    "                \n",
    "    # 最後に NumPy 配列へ変換（NTIME × N の形になる）\n",
    "    payoff_history = np.array(payoff_history)\n",
    "\n",
    "    # payoff_history（生データ）をファイル保存\n",
    "    np.savetxt(f\"payoff_history_vs2_all_trial_{trial}.txt\", payoff_history, fmt=\"%.6f\", delimiter=\",\")\n",
    "\n",
    "    # アクションカウントをメモに保存\n",
    "    with open(\"action_counts_summary_vs2_all.txt\", \"w\") as memo_file:\n",
    "        memo_file.write(\"Action Counts Summary:\\n\")\n",
    "        memo_file.write(f\"x_ov: {action_counts['x_ov']}\\n\")\n",
    "        memo_file.write(f\"x_und: {action_counts['x_und']}\\n\")\n",
    "        memo_file.write(f\"xc: {action_counts['xc']}\\n\")\n",
    "        memo_file.write(f\"xn: {action_counts['xn']}\\n\")\n",
    "\n",
    "    return results_1, np.array(prob_history), payoff_history, t+1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    N = 3  # エージェント数\n",
    "    NTIME = 100000000  # タイムステップ数\n",
    "    a, b, c, xmax = 2.0, 1.0, 1.0, 2.5\n",
    "    chi, kappa = 1.0, 0.0\n",
    "\n",
    "    for trial in range(1, 101):  # 100回シミュレーション \n",
    "        print(f\"=== Trial {trial} / 100 ===\")\n",
    "        results, prob_history, payoff_history, s = run_simulation(N, NTIME, a, b, c, xmax, chi, kappa, trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
